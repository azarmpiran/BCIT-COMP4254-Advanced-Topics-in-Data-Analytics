{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a59e26cc",
   "metadata": {},
   "source": [
    "<h1><center>Dimension Reduction Tutorial</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6755a84e",
   "metadata": {},
   "source": [
    "## What is Dimension Reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a69613",
   "metadata": {},
   "source": [
    "Dimension reduction is the process of eliminating variables that has less effect on target and finding the most relevant and meaningful properties of the dataset. Sometimes, we work on a dataset with more than a thousands features and it is very hard to analyze each and every variable. Dimension reduction helps us to resize the data to a low-dimensional space to be able to easily extract patterns and insights.\n",
    "\n",
    "The Divorce dataset that we used in the first part of this project has 54 features which makes it difficult to know which predictors we need to use for our model. In the first part of the project I used chi-square test to find significant predictors and I chose the 8 variables with highest score. \n",
    "\n",
    "In this part, we will go through other methods of dimension reduction and compare the result of each method to see which works better. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18fc4db",
   "metadata": {},
   "source": [
    "## Why is Dimension Reduction required?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7481f2",
   "metadata": {},
   "source": [
    "Here are three benefits of applying dimension reduction techniques to a dataset :\n",
    "\n",
    "1) Applying dimension reduction will reduce the space we need to store the data.\n",
    "\n",
    "2) Less dimension means less processing and computation time.\n",
    "\n",
    "3) Visualizing data will be easier with less dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a389e1a",
   "metadata": {},
   "source": [
    "## Common Dimension Reduction Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b46bfe1",
   "metadata": {},
   "source": [
    "### Missing Value Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9198a2",
   "metadata": {},
   "source": [
    "This method is about removing columns with lots of missing values. Imagine a column that ore than a half of it is null, even if impute the column it will not give us usufull information so it is better to just get rid of it. \n",
    "\n",
    "as a result of running X.isnull().sum()/len(X)*100 we will see that since we do not have any null value, all the numbers will be zero. so we can say this technique is not usefull in this case at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7355880b",
   "metadata": {},
   "source": [
    "### Low Variance Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62aa03a",
   "metadata": {},
   "source": [
    "Imagine an attribute in dataset that all the records has the same value, this column will not improve any result in our model. In our case, lets say everyone answered Never = 0 to one of the questions. A column with very low variance will not be helpful and we prefer to drop it. \n",
    "\n",
    "First, we need to calculate the variance so we need to make sure that there is no null in data. In my case there is no null so I just calculated the variance.\n",
    "\n",
    "As a result of running X.var() we will see that again this is not a good method since the variance of all column are so close to each other and it is hard to decide which one to remove."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa524ec",
   "metadata": {},
   "source": [
    "### High Correlation Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab48a5e",
   "metadata": {},
   "source": [
    "Some of the questions might carry the same meaning for couples and their answers might be the same. In this case the correlation of those question will be very close to one, so I believe this might be a good method to filter some attributes in dataset. After finding attributes with a high correlation, we can just remove one of them and stick to the other for our model. \n",
    "\n",
    "After running X.corr() we can see that since we have 54 attributes it will not be a good idea to check all these numbers so I do not prefer this method as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5256a995",
   "metadata": {},
   "source": [
    "### Backward Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b09d2a5",
   "metadata": {},
   "source": [
    "Follow the below steps to understand and use the ‘Backward Feature Elimination’ technique: (https://www.analyticsvidhya.com/blog/)\n",
    "\n",
    "1) We first take all the n variables present in our dataset and train the model using them\n",
    "\n",
    "2) We then calculate the performance of the model\n",
    "\n",
    "3) Now, we compute the performance of the model after eliminating each variable (n times), i.e., we drop one variable every time and train the model on the remaining n-1 variables\n",
    "\n",
    "4) We identify the variable whose removal has produced the smallest (or no) change in the performance of the model, and then drop that variable\n",
    "\n",
    "5) Repeat this process until no variable can be dropped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932849db",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "082f7f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "FEATUERS SELECTED\n",
      "\n",
      "\n",
      "[False False  True False False  True False False False False False False\n",
      " False False False False  True  True False False False False False False\n",
      " False  True False False False False False False False False False False\n",
      " False False  True  True False False False False False False False False\n",
      "  True False False False False False]\n",
      "\n",
      "\n",
      "RANKING OF FEATURES\n",
      "\n",
      "\n",
      "[13  9  1 28 24  1 47 38 33 41 29 27 32 17  8 26  1  1  6  5 31 44 42 37\n",
      " 25  1 30  2 15 11  3 19 22 12 36 18 43 14  1  1 16 23 20  7 45 40 34 35\n",
      "  1 21 46  4 10 39]\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries:\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from   sklearn  import metrics\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   sklearn.linear_model    import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Read the data:\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Azarm\\\\Desktop\\\\BCIT\\\\AdvancedTopics\\\\DataSets\\\\Divorce.csv\",header = 0)\n",
    "\n",
    "# Seperate the target and independent variable\n",
    "X = df.copy()       # Create separate copy to prevent unwanted tampering of data.\n",
    "del X['Divorce']     # Delete target variable.\n",
    "\n",
    "# Target variable\n",
    "y = df['Divorce']\n",
    "\n",
    "# Create the object of the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Specify the number of  features to select \n",
    "rfe = RFE(model, 8)\n",
    "\n",
    "# fit the model\n",
    "rfe = rfe.fit(X, y)\n",
    "\n",
    "# Please uncomment the following lines to see the result\n",
    "print('\\n\\nFEATUERS SELECTED\\n\\n')\n",
    "print(rfe.support_)\n",
    "\n",
    "print('\\n\\nRANKING OF FEATURES\\n\\n')\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34df382",
   "metadata": {},
   "source": [
    "#### Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e60795",
   "metadata": {},
   "source": [
    "|    | <div style=\"text-align: left\">Attribute Name</div>| <div style=\"text-align: left\">Information</div>                                                                                                 \n",
    "|----|---------------|---------------------------------------------------------------------------------------------------------\n",
    "| 1  | Q3            | <div style=\"text-align: left\">When we need it, we can take our discussions with my spouse from the beginning and correct it.</div>                                                                        \n",
    "| 2  | Q6           |  <div style=\"text-align: left\">We don't have time at home as partners.</div>                                                                         \n",
    "| 3  | ***Q17            |  <div style=\"text-align: left\">We share the same views about being happy in our life with my spouse </div>               \n",
    "| 4  | ***Q18            |  <div style=\"text-align: left\">My spouse and I have similar ideas about how marriage should be</div>                                \n",
    "| 5  | Q26            |  <div style=\"text-align: left\">I know my spouse's basic anxieties.</div>                \n",
    "| 6  | Q39            |  <div style=\"text-align: left\">Our discussions often occur suddenly.</div>                        \n",
    "| 7  | ***Q40            |  <div style=\"text-align: left\">We're just starting a discussion before I know what's going on. \n",
    "| 8  |Q49            |  <div style=\"text-align: left\">We're just starting a discussion before I know what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00af4320",
   "metadata": {},
   "source": [
    "#### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "286d408a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results without scaling:\n",
      "\n",
      "Confusion Matrix\n",
      "Predicted   0   1\n",
      "Actual           \n",
      "0          22   0\n",
      "1           1  20\n",
      "\n",
      "Precision:  1.0\n",
      "Recall:     0.952\n",
      "F1:         0.976\n",
      "Accuracy:  0.9767441860465116\n"
     ]
    }
   ],
   "source": [
    "# Implementing Logistic Regression\n",
    "\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   sklearn.linear_model    import LogisticRegression\n",
    "\n",
    "# Re-assign X with significant columns only after chi-square test.\n",
    "X = X[['Q3', 'Q6', 'Q17','Q18','Q26','Q39','Q40','Q49']]\n",
    "\n",
    "y = df['Divorce']\n",
    "\n",
    "# Split data.\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.25,random_state=0)\n",
    "\n",
    "# Perform logistic regression.\n",
    "logisticModel = LogisticRegression(fit_intercept=True, solver='liblinear',random_state=0)\n",
    "\n",
    "# Fit the model.\n",
    "logisticModel.fit(X_train,y_train)\n",
    "y_pred=logisticModel.predict(X_test)\n",
    "# print(y_pred)\n",
    "\n",
    "# Show accuracy scores.\n",
    "print('Results without scaling:')\n",
    "\n",
    "# Show confusion matrix\n",
    "cm = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(cm)\n",
    "\n",
    "TN = cm[0][0] # True Negative  (Col 0, Row 0)\n",
    "FN = cm[0][1] # False Negative (Col 0, Row 1)\n",
    "FP = cm[1][0] # False Positive (Col 1, Row 0)\n",
    "TP = cm[1][1] # True Positive  (Col 1, Row 1)\n",
    "\n",
    "\n",
    "precision = (TP/(FP + TP))\n",
    "print(\"\\nPrecision:  \" + str(round(precision, 3)))\n",
    "\n",
    "recall = (TP/(TP + FN))\n",
    "print(\"Recall:     \" + str(round(recall,3)))\n",
    "\n",
    "F1 = 2*((precision*recall)/(precision+recall))\n",
    "print(\"F1:         \" + str(round(F1,3)))\n",
    "\n",
    "print('Accuracy: ',metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b309001",
   "metadata": {},
   "source": [
    "#### Cross Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d83c311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy and Standard Deviation For All Folds:\n",
      "Average accuracy:  0.9882352941176471\n",
      "Accuracy std:      0.014408763192842228\n",
      "Average precision: 1.0\n",
      "Precision std:     0.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import numpy as np\n",
    "\n",
    "# enumerate splits - returns train and test arrays of indexes.\n",
    "# scikit-learn k-fold cross-validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# prepare cross validation with eight folds.\n",
    "kfold         = KFold(5, True)\n",
    "accuracyList  = []\n",
    "precisionList = []\n",
    "f1List        = []\n",
    "recallList = []\n",
    "count         = 0\n",
    "\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    X_train = X.loc[X.index.isin(train_index)]\n",
    "    X_test  = X.loc[X.index.isin(test_index)]\n",
    "    y_train = y.loc[y.index.isin(train_index)]\n",
    "    y_test  = y.loc[y.index.isin(test_index)]\n",
    "\n",
    "    # Perform logistic regression.\n",
    "    logisticModel = LogisticRegression(fit_intercept=True,\n",
    "                                       solver='liblinear')\n",
    "    # Fit the model.\n",
    "    logisticModel.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = logisticModel.predict(X_test)\n",
    "\n",
    "    # Show confusion matrix and accuracy scores.\n",
    "    cm = pd.crosstab(y_test, y_pred,\n",
    "                     rownames=['Actual'],\n",
    "                     colnames=['Predicted'])\n",
    "    count += 1\n",
    "\n",
    "    # Calculate accuracy and precision scores and add to the list.\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "\n",
    "    accuracyList.append(accuracy)\n",
    "    precisionList.append(precision)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "    accuracyList.append(accuracy)\n",
    "    precisionList.append(precision)\n",
    "    recallList.append(recall)\n",
    "    f1List.append(f1)\n",
    "\n",
    "\n",
    "\n",
    "# Show averages of scores over multiple runs.\n",
    "print(\"\\nAccuracy and Standard Deviation For All Folds:\")\n",
    "print(\"Average accuracy:  \" + str(np.mean(accuracyList)))\n",
    "print(\"Accuracy std:      \" + str(np.std(accuracyList)))\n",
    "print(\"Average precision: \" + str(np.mean(precisionList)))\n",
    "print(\"Precision std:     \" + str(np.std(precisionList)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93529a24",
   "metadata": {},
   "source": [
    "### Forward Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2417b01",
   "metadata": {},
   "source": [
    "This is the opposite process of the Backward Feature Elimination we saw above. Instead of eliminating features, we try to find the best features which improve the performance of the model. This technique works as follows: (https://www.analyticsvidhya.com/blog/)\n",
    "\n",
    "1) We start with a single feature. \n",
    "\n",
    "2) Essentially, we train the model n number of times using each feature separately\n",
    "\n",
    "3) The variable giving the best performance is selected as the starting variable\n",
    "\n",
    "4) Then we repeat this process and add one variable at a time. The variable that produces the highest increase in performance is retained\n",
    "\n",
    "5) We repeat this process until no significant improvement is seen in the model’s performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121207d8",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06f3f7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q9', 'Q11', 'Q15', 'Q17', 'Q18', 'Q19', 'Q20', 'Q40']\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries:\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from   sklearn  import metrics\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   sklearn.linear_model    import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "# Read the data:\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Azarm\\\\Desktop\\\\BCIT\\\\AdvancedTopics\\\\DataSets\\\\Divorce.csv\",header = 0)\n",
    "\n",
    "# Seperate the target and independent variable\n",
    "X = df.copy()       # Create separate copy to prevent unwanted tampering of data.\n",
    "del X['Divorce']     # Delete target variable.\n",
    "\n",
    "# Target variable\n",
    "y = df['Divorce']\n",
    "\n",
    "#  f_regression is a scoring function to be used in a feature selection procedure\n",
    "#  f_regression will compute the correlation between each regressor and the target \n",
    "ffs = f_regression(X,y )\n",
    "\n",
    "variable = [ ]\n",
    "for i in range(0,len(X.columns)-1):\n",
    "    if ffs[0][i] >=700:\n",
    "       variable.append(X.columns[i])\n",
    "    \n",
    "print(variable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c1f6ad",
   "metadata": {},
   "source": [
    "#### Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81302e5a",
   "metadata": {},
   "source": [
    "|    | <div style=\"text-align: left\">Attribute Name</div>| <div style=\"text-align: left\">Information</div>                                                                                                 \n",
    "|----|---------------|---------------------------------------------------------------------------------------------------------\n",
    "| 1  | Q9            | <div style=\"text-align: left\">I enjoy traveling with my wife.</div>                                                                        \n",
    "| 2  | Q11           |  <div style=\"text-align: left\">I think that one day in the future, when I look back, I see that my spouse and I have been in harmony with each other.</div>                                                                         \n",
    "| 3  | Q15            |  <div style=\"text-align: left\">Our dreams with my spouse are similar and harmonious. </div>               \n",
    "| 4  | ***Q17            |  <div style=\"text-align: left\">We share the same views about being happy in our life with my spouse.</div>                                \n",
    "| 5  | ***Q18            |  <div style=\"text-align: left\">My spouse and I have similar ideas about how marriage should be.</div>                \n",
    "| 6  | Q19           |  <div style=\"text-align: left\">My spouse and I have similar ideas about how roles should be in marriage</div>                        \n",
    "| 7  | Q20            |  <div style=\"text-align: left\">My spouse and I have similar values in trust.</div>\n",
    "| 8  |***Q40            |  <div style=\"text-align: left\">We're just starting a discussion before I know what's going on.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a963f",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be7dc99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results without scaling:\n",
      "\n",
      "Confusion Matrix\n",
      "Predicted   0   1\n",
      "Actual           \n",
      "0          22   0\n",
      "1           2  19\n",
      "\n",
      "Precision:  1.0\n",
      "Recall:     0.905\n",
      "F1:         0.95\n",
      "Accuracy:  0.9534883720930233\n"
     ]
    }
   ],
   "source": [
    "# Implementing Logistic Regression\n",
    "\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   sklearn.linear_model    import LogisticRegression\n",
    "\n",
    "# Re-assign X with significant columns only after chi-square test.\n",
    "X = X[['Q9', 'Q11', 'Q15','Q17','Q18','Q19','Q20','Q40']]\n",
    "\n",
    "y = df['Divorce']\n",
    "\n",
    "# Split data.\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.25,random_state=0)\n",
    "\n",
    "# Perform logistic regression.\n",
    "logisticModel = LogisticRegression(fit_intercept=True, solver='liblinear',random_state=0)\n",
    "\n",
    "# Fit the model.\n",
    "logisticModel.fit(X_train,y_train)\n",
    "y_pred=logisticModel.predict(X_test)\n",
    "# print(y_pred)\n",
    "\n",
    "# Show accuracy scores.\n",
    "print('Results without scaling:')\n",
    "\n",
    "# Show confusion matrix\n",
    "cm = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(cm)\n",
    "\n",
    "TN = cm[0][0] # True Negative  (Col 0, Row 0)\n",
    "FN = cm[0][1] # False Negative (Col 0, Row 1)\n",
    "FP = cm[1][0] # False Positive (Col 1, Row 0)\n",
    "TP = cm[1][1] # True Positive  (Col 1, Row 1)\n",
    "\n",
    "\n",
    "precision = (TP/(FP + TP))\n",
    "print(\"\\nPrecision:  \" + str(round(precision, 3)))\n",
    "\n",
    "recall = (TP/(TP + FN))\n",
    "print(\"Recall:     \" + str(round(recall,3)))\n",
    "\n",
    "F1 = 2*((precision*recall)/(precision+recall))\n",
    "print(\"F1:         \" + str(round(F1,3)))\n",
    "\n",
    "print('Accuracy: ',metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf35bf5",
   "metadata": {},
   "source": [
    "#### Cross Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ada7634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy and Standard Deviation For All Folds:\n",
      "Average accuracy:  0.9823529411764707\n",
      "Accuracy std:      0.02352941176470589\n",
      "Average precision: 1.0\n",
      "Precision std:     0.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import numpy as np\n",
    "\n",
    "# enumerate splits - returns train and test arrays of indexes.\n",
    "# scikit-learn k-fold cross-validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# prepare cross validation with eight folds.\n",
    "kfold         = KFold(5, True)\n",
    "accuracyList  = []\n",
    "precisionList = []\n",
    "f1List        = []\n",
    "recallList = []\n",
    "count         = 0\n",
    "\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    X_train = X.loc[X.index.isin(train_index)]\n",
    "    X_test  = X.loc[X.index.isin(test_index)]\n",
    "    y_train = y.loc[y.index.isin(train_index)]\n",
    "    y_test  = y.loc[y.index.isin(test_index)]\n",
    "\n",
    "    # Perform logistic regression.\n",
    "    logisticModel = LogisticRegression(fit_intercept=True,\n",
    "                                       solver='liblinear')\n",
    "    # Fit the model.\n",
    "    logisticModel.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = logisticModel.predict(X_test)\n",
    "\n",
    "    # Show confusion matrix and accuracy scores.\n",
    "    cm = pd.crosstab(y_test, y_pred,\n",
    "                     rownames=['Actual'],\n",
    "                     colnames=['Predicted'])\n",
    "    count += 1\n",
    "\n",
    "    # Calculate accuracy and precision scores and add to the list.\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "\n",
    "    accuracyList.append(accuracy)\n",
    "    precisionList.append(precision)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "    accuracyList.append(accuracy)\n",
    "    precisionList.append(precision)\n",
    "    recallList.append(recall)\n",
    "    f1List.append(f1)\n",
    "\n",
    "\n",
    "\n",
    "# Show averages of scores over multiple runs.\n",
    "print(\"\\nAccuracy and Standard Deviation For All Folds:\")\n",
    "print(\"Average accuracy:  \" + str(np.mean(accuracyList)))\n",
    "print(\"Accuracy std:      \" + str(np.std(accuracyList)))\n",
    "print(\"Average precision: \" + str(np.mean(precisionList)))\n",
    "print(\"Precision std:     \" + str(np.std(precisionList)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b554721a",
   "metadata": {},
   "source": [
    "### Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc455ac7",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfa582e",
   "metadata": {},
   "source": [
    "In this tutorial, we went through 5 different methods of dimension reduction. I would like to mention that BFE and FFS are both time consuming and expensive in terms of computation. It will be better to use them in case we have a small number of input variables.\n",
    "\n",
    "The best method of dimension reduction in Divorce dataset based on the result was Backward Feature Elimination. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296f7ffc",
   "metadata": {},
   "source": [
    "### Camparison Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b959e83",
   "metadata": {},
   "source": [
    "|<div style=\"text-align: left\">Dimension Reduction Method</div>|<div style=\"text-align: left\">Selected Attributes</div>|<div style=\"text-align: left\">Model Accuracy</div>|<div style=\"text-align: left\">Cross Fold Average Accuracy</div>                                                                                             \n",
    "|----|---------------|-------|------------------------------------------------------------------------------------------------\n",
    "|<div style=\"text-align: left\">Chi-Square Test</div>|  <div style=\"text-align: left\">Q5, Q9, Q17, Q18, Q19, Q35, Q36, Q40</div>      | <div style=\"text-align: center\">0.9534883720930233</div> | <div style=\"text-align: center\">0.9823529411764707</div>\n",
    "|<div style=\"text-align: left\">Backward Feature Elimination</div>|  <div style=\"text-align: left\">Q3, Q6, Q17, Q18, Q26, Q39, Q40, Q49</div>  | <div style=\"text-align: center\">0.9767441860465116</div> | <div style=\"text-align: center\">0.9882352941176471</div>|\n",
    "|<div style=\"text-align: left\">Forward Feature Selection</div>|  <div style=\"text-align: left\">Q9, Q11, Q15, Q17, Q18, Q19, Q20, Q40</div> | <div style=\"text-align: center\">0.9534883720930233</div> |<div style=\"text-align: center\">0.9823529411764707</div> |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92beae53",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c19ee0",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/\n",
    "\n",
    "https://www.datacamp.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "479.6px",
    "left": "24px",
    "top": "200.8px",
    "width": "255.969px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
